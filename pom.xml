<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>org.example</groupId>
    <artifactId>eas-017-spark-fundamentals-java</artifactId>
    <version>1.0-SNAPSHOT</version>

    <properties>
        <scala.compile.major.version>2.12</scala.compile.major.version>
        <spark.major.version>3.0</spark.major.version>
        <spark.version>${spark.major.version}.3</spark.version>
        <hadoop.version>3.2.0</hadoop.version>
    </properties>

     <dependencies>
         <!-- spark-core is enough to start -->
         <dependency>
             <groupId>org.apache.spark</groupId>
             <artifactId>spark-core_${scala.compile.major.version}</artifactId>
             <version>${spark.version}</version>
             <!--<scope>provided</scope>--> <!-- Scope compile is required for local Spark launch  -->
         </dependency>

         <!-- We will use NullPrintStream from there -->
         <dependency>
             <groupId>commons-io</groupId>
             <artifactId>commons-io</artifactId>
             <version>2.11.0</version>
         </dependency>

         <!--  required to access AWS s3 -->
         <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>${hadoop.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-aws</artifactId>
            <version>${hadoop.version}</version>
        </dependency>

         <!-- spark-sql contains Dataset API -->
         <dependency>
             <groupId>org.apache.spark</groupId>
             <artifactId>spark-sql_${scala.compile.major.version}</artifactId>
             <version>${spark.version}</version>
         </dependency>

         <!-- Graph libraries for day 5 -->
         <dependency>
             <groupId>org.apache.spark</groupId>
             <artifactId>spark-graphx_${scala.compile.major.version}</artifactId>
             <version>${spark.version}</version>
         </dependency>
         <dependency>
             <groupId>graphframes</groupId>
             <artifactId>graphframes</artifactId>
             <version>0.8.1-spark${spark.major.version}-s_${scala.compile.major.version}</version>
         </dependency>
     </dependencies>

     <repositories>
        <!-- For graphframes -->
         <repository>
             <id>spark-packages</id>
             <url>https://repos.spark-packages.org/</url>
         </repository>
     </repositories>

      <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.7.0</version>
                <configuration>
                    <source>1.8</source>
                    <target>1.8</target>
                </configuration>
            </plugin>
            <!-- Packaging Hadoop dependencies in uber-jar for easy cluster launch -->
<!--            <plugin>-->
<!--                <groupId>org.apache.maven.plugins</groupId>-->
<!--                <artifactId>maven-shade-plugin</artifactId>-->
<!--                <version>3.2.4</version>-->
<!--                <executions>-->
<!--                    <execution>-->
<!--                        <phase>package</phase>-->
<!--                        <goals>-->
<!--                            <goal>shade</goal>-->
<!--                        </goals>-->
<!--                    </execution>-->
<!--                </executions>-->
<!--          </plugin>-->
        </plugins>
    </build>
    
</project>